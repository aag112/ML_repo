{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictWebTraffic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPl4qeIRPX4QSf34GnlAwPG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og0tHrhCFEpp"
      },
      "source": [
        "Using Time Series, predict web traffic. CSV file obtained from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKpJbI64Eu3Z"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pylab as p\n",
        "import matplotlib.pyplot as plot\n",
        "from collections import Counter\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTv-2zwjE0uP"
      },
      "source": [
        "data=pd.read_csv (\"train_1.csv\")\n",
        "\n",
        "data.info()\n",
        "data.describe()\n",
        "n=data.isnull().sum()\n",
        "data_no_null=data.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxHqH18E0rL"
      },
      "source": [
        "import re\n",
        "def language_abb(page): \n",
        "    res = re.search('[a-z][a-z].wikipedia.org',page) \n",
        "    if res: \n",
        "        return res.group(0)[0:2] \n",
        "    return 'na'\n",
        "\n",
        "data['lang'] = data.Page.map(language_abb)\n",
        "data[\"lang\"]= data[\"lang\"].replace([\"en\",\"ja\",'de','na','fr','zh','ru','es'],\n",
        "                                   [\"English\",\"Japanese\",'German','WikiMedia','French','Chinese','Russian','Spanish'])\n",
        "\n",
        "lang_count=data['lang'].value_counts()\n",
        "\n",
        "lang_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOhNIENNE0oD"
      },
      "source": [
        "# Grouping df by langauge\n",
        "name=[\"en\",\"ja\",'de','na','fr','zh','ru','es']\n",
        "full_name=[\"English\",\"Japanese\",'German','WikiMedia','French','Chinese','Russian','Spanish']\n",
        "lang_sets={}\n",
        "for x in range(0,8):\n",
        "    lang_sets[name[x]] = data[data.lang==full_name[x]].iloc[:,0:-1]\n",
        "# Removing the page names\n",
        "for index in lang_sets:\n",
        "    lang_sets[index]=lang_sets[index].drop(columns=['Page'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Emb_eDE0jw"
      },
      "source": [
        "null_num={}\n",
        "t_sum={}\n",
        "mean={}\n",
        "meanperDay={}\n",
        "\n",
        "\n",
        "for index in lang_sets:\n",
        "    null_num[index]=lang_sets[index].isnull().sum()\n",
        "    t_sum[index]=null_num[index].sum(axis=0)/550\n",
        "    mean[index]=lang_sets[index].mean().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRpQF0wPE0g_"
      },
      "source": [
        "# Dealing with Null Values\n",
        "for index in lang_sets:\n",
        "    lang_sets[index]=lang_sets[index].fillna(0)\n",
        "                                             \n",
        "# Summing for different langues/ reshaping the input data\n",
        "sum_col ={}\n",
        "\n",
        "for index in lang_sets:\n",
        "    sum_col[index]=lang_sets[index].sum(axis=0) / lang_sets[index].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG1hFlR_E0cd"
      },
      "source": [
        "# Extracting data according to the language given in the input\n",
        "\n",
        "import math as math\n",
        "\n",
        "def extract_language(df):\n",
        "    #size = int(len(df)-50)\n",
        "    #train_ds, test_ds = df[0:size], df[size:len(df)]\n",
        "    df_train = pd.DataFrame({ 'Date':(df.index),'Hits': (df.values)})\n",
        "    #df_test = pd.DataFrame({ 'Date':(test_ds.index),'Hits': (test_ds.values)})\n",
        "    \n",
        "    \n",
        "    return df_train\n",
        "\n",
        "X_train = extract_language(sum_col[\"en\"])# Change this one by on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMq8_rm-E0ZY"
      },
      "source": [
        "def timeseries_to_supervised(data, lag=1):\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
        "\tcolumns.append(df)\n",
        "\tdf = pd.concat(columns, axis=1)\n",
        "\tdf.fillna(0, inplace=True)\n",
        "\treturn df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CB0lsceE0Wp"
      },
      "source": [
        "def fit_lstm(X_train,y_train,batch_size, nb_epoch, neurons):\n",
        "\t#X, y = train[:, 0:-1], train[:, -1]\n",
        "\n",
        "  n=2\n",
        "  func =  'softmax'\n",
        "  X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(neurons, batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), stateful=True))\n",
        "  model.add(Dense(n,activation=func))\n",
        "  model.add(Dense(n,activation=func))\n",
        "  model.add(Dense(n,activation=func))\n",
        "  model.add(Dense(n,activation=func))\n",
        "\n",
        "\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error',optimizer = 'adam')\n",
        "  for i in range(nb_epoch):\n",
        "    print(\"nb_epochs = \",i)\n",
        "    model.fit(X_train,y_train,epochs=1,batch_size = batch_size , verbose =0,shuffle=False)\n",
        "    model.reset_states()\n",
        "\n",
        "  return model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lrF_D1vE0SX"
      },
      "source": [
        "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "def scaling_windowing(df_train,window_size):\n",
        "  from sklearn.preprocessing import StandardScaler,MinMaxScaler  \n",
        "  scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "  df_supervised = timeseries_to_supervised(df_train['Hits'].values,window_size)\n",
        "  train_data = df_supervised[0:500].values\n",
        "  test_data = df_supervised[500:550].values\n",
        "  print(df_supervised.shape)\n",
        "\n",
        "  train_data_scaled = scaler.fit_transform(train_data)  \n",
        "  test_data_scaled = scaler.fit_transform(test_data)\n",
        "  X_train,y_train = train_data_scaled[:,0:window_size] , train_data_scaled[:,window_size: window_size+1]\n",
        "  X_test,y_test = test_data_scaled[:,0:window_size] , test_data_scaled[:,window_size:window_size+1]\n",
        " \n",
        "  print(\"X_train.shape = \", X_train.shape)\n",
        "  print(\"y_train.shape = \", y_train.shape)\n",
        "\n",
        "  print(\"X_test.shape = \",X_test.shape)\n",
        "  print(\"y_test.shape = \",y_test.shape)\n",
        "\n",
        "\n",
        "  print(\"shape = \",df_supervised)\n",
        "  return [X_train,y_train,X_test,y_test,train_data,test_data,scaler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNLfZbYxE0P0"
      },
      "source": [
        "def forecast_lstm(model, batch_size, X):\n",
        "\tX = X.reshape(1, 1, len(X))\n",
        "\tyhat = model.predict(X, batch_size=batch_size)\n",
        "\treturn yhat[0,0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def invert_scale(scaler,X,value):\n",
        "  new_row = [x for x in X] + [value]\n",
        "  array = np.array(new_row)\n",
        "  array = array.reshape(1,len(array))\n",
        "  print(\"array.shape  = \",array.shape)\n",
        "  inverted = scaler.inverse_transform(array)\n",
        "  return inverted[0,-1]\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "def prediction(lstm_model,X_test,scaler):\n",
        "  predictions = []\n",
        "  for i in range(len(X_test)):\n",
        "      X = X_test[i,:] \n",
        "      y_knot = forecast_lstm(lstm_model,1,X)\n",
        "      y_pred = invert_scale(scaler,X,y_knot)\n",
        "      predictions.append(y_pred)\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QZAb5tpE0NJ"
      },
      "source": [
        "def plot_graphs(y_pred,y_actual):\n",
        "  import matplotlib.pyplot as plt\n",
        "  y_actual = list(y_actual)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  time = [x for x in range(0,len(y_actual))]\n",
        "  labels={'Orginal','Predicted'}\n",
        "  plt.plot(time, y_actual, color= 'green')\n",
        "  plt.plot(time,y_pred, color = 'orange')\n",
        "  plt.title('Actual Vs Forecast- LSTM')\n",
        "  plt.xlabel('Days')\n",
        "  plt.ylabel('Hits')\n",
        "  plt.legend(labels)\n",
        "  plt.show()\n",
        "  \n",
        "def print_side_by_side(y_pred,y_actual):\n",
        "    for i in range(0,len(y_pred)):\n",
        "      print(y_pred[i], \" , \", y_actual[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHYh2U_sE0Kl"
      },
      "source": [
        "error_rmse=[]\n",
        "error_log_mse=[]\n",
        "error_pd=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeLgQMRyE0IO"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "window_size = 4\n",
        "\n",
        "nb=[200]\n",
        "df_train = extract_language(sum_col[\"ja\"])\n",
        "print(\"df_train = \",df_train.head())\n",
        "\n",
        "\n",
        "def percentage_diff(y_p,y_a):\n",
        "  error=0\n",
        "  for i in range(0,len(y_a)):\n",
        "    error = (error + (float(abs (y_p[i] - y_a [i])) / float(y_a[i]) * 100 ) )\n",
        "  error = error/ len(y_a)\n",
        "  return error\n",
        "\n",
        "\n",
        "for i in range(0,len(nb)):\n",
        "  \n",
        "    [X_train,y_train,X_test,y_test,train_data,test_data,scaler]=scaling_windowing(df_train,window_size)\n",
        "    lstm_model = fit_lstm(X_train,y_train, 1,nb[i], 4)\n",
        "    y_pred = prediction(lstm_model,X_test,scaler)\n",
        "    y_actual= list(test_data[:,window_size : window_size+1])\n",
        "    print_side_by_side(y_pred,y_actual)\n",
        "    plot_graphs(y_pred,y_actual)\n",
        "    rmse = sqrt(mean_squared_error(y_actual,y_pred))\n",
        "    log_mse = np.log(np.array(mean_squared_error(y_actual,y_pred)))\n",
        "    pdiff = percentage_diff(y_pred,y_actual)\n",
        "    print('Test RMSE: %.3f' % rmse)\n",
        "    print('Test log-MSE: %.3f' % log_mse)\n",
        "    print('Percentage difference: %.3f' % pdiff)\n",
        "    error_rmse.append(rmse)\n",
        "    error_log_mse.append(log_mse)\n",
        "    error_pd.append(pdiff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Z3MJfKHOYg"
      },
      "source": [
        "Test RMSE: 97.977\n",
        "Test log-MSE: 9.169\n",
        "Percentage difference: 9.281"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa-c15wPE0FF"
      },
      "source": [
        "funcc=['tanh','sigmoid','relu','softmax']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7vjOWshE0Br"
      },
      "source": [
        "nb=[1,2,3,4,5,6,7,8]\n",
        "\n",
        "\n",
        "fig = plot.figure(1,figsize=[5,5])\n",
        "plot.ylabel('Percentage Difference')\n",
        "plot.xlabel('Functions')\n",
        "plot.title('Percentage Difference vs Activation Functions ')\n",
        "plot.bar(funcc,error_pd,width=0.5,color='b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEEykYT-Ez_r"
      },
      "source": [
        "fig = plot.figure(1,figsize=[10,5])\n",
        "plot.ylabel('RMSE')\n",
        "plot.xlabel('Number of Epochs')\n",
        "plot.plot(nb[2:len(nb)],error_rmse[2:len(nb)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc26ws-zEz9k"
      },
      "source": [
        "fig = plot.figure(1,figsize=[10,5])\n",
        "plot.ylabel('Percentage Difference %')\n",
        "plot.xlabel('Number of Epochs')\n",
        "plot.plot(nb[2:len(nb)-1],error_pd[2:len(nb)-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdsPOQb0Ez50"
      },
      "source": [
        "fig = plot.figure(1,figsize=[10,5])\n",
        "plot.ylabel('Log Mean Square Error')\n",
        "plot.xlabel('Number of Epochs')\n",
        "plot.plot(nb[2:len(nb)-1],error_log_mse[2:len(nb)-1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3kixhkkEz3a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}